{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p5iMIOl37EDn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nitis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wd1qLQDf9m9f"
   },
   "outputs": [],
   "source": [
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZuCRDo1i9sxh"
   },
   "outputs": [],
   "source": [
    "# Authenticate and create the PyDrive client.\n",
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3kif9vK47Sg8"
   },
   "outputs": [],
   "source": [
    "# link = \"https://drive.google.com/open?id=1yvfGNLWo3_yQBDkiHmiv9yMeXC2Gxhnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "PU5v9yBf7lUH",
    "outputId": "6ce18b82-f711-4448-97ba-3815e3531c8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1yvfGNLWo3_yQBDkiHmiv9yMeXC2Gxhnn\n"
     ]
    }
   ],
   "source": [
    "# fluff, id = link.split('=')\n",
    "# print (id) # Verify that you have everything after '='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ekOL0KCL-I2F"
   },
   "outputs": [],
   "source": [
    "# downloaded = drive.CreateFile({'id':id}) \n",
    "# downloaded.GetContentFile('train.tsv')  \n",
    "# df = pd.read_csv('train.tsv', sep = '\\t')\n",
    "# # Dataset is now stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MRDB_5rgTFbM"
   },
   "outputs": [],
   "source": [
    "NUM_BRANDS = 2000\n",
    "MAX_NUM_WORDS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mr4Dv05-TrSE"
   },
   "outputs": [],
   "source": [
    "def rmsle(y_pred, y_test):\n",
    "  \"\"\" root mean square logrithmic error \"\"\"\n",
    "  assert len(y_pred) == len(y_test)\n",
    "  return np.sqrt(np.mean(np.power(y_pred-y_test ,2)))\n",
    "\n",
    "def readData():\n",
    "    df = pd.read_csv('..\\\\mercari-price-suggestion-challenge\\\\train.tsv', sep='\\t', encoding='utf-8')\n",
    "    df.drop(columns='train_id', inplace=True)\n",
    "    return df\n",
    "\n",
    "def fillMissingData(df):\n",
    "  \"\"\" fills the NA values in the dataset \"\"\"\n",
    "  df.brand_name.fillna(value='', inplace=True)\n",
    "  df.category_name.fillna(value='//', inplace=True)\n",
    "  df.item_description.fillna(value='', inplace=True)\n",
    "  df.item_description = df.item_description.apply(lambda x : x.replace(\"No description yet\", ''))\n",
    "  print(\"*******Filled Missing Data*******\")\n",
    "    \n",
    "# Creating three new columns to separate the categories \n",
    "def splittingCategories(df):\n",
    "    df['primary_category'] = df.category_name.apply(lambda cat:cat.split('/')[0].strip())\n",
    "    df['sub_category1'] = df.category_name.apply(lambda cat:cat.split('/')[1].strip())\n",
    "    df['sub_category2'] = df.category_name.apply(lambda cat:cat.split('/')[2].strip())\n",
    "    return df\n",
    "    \n",
    "def fillBrandNames(x):\n",
    "    \"\"\"Fills the brand names using the name column of the dataset\"\"\"\n",
    "    try:\n",
    "        nameList = []\n",
    "        for i in [4,3,2,1]:\n",
    "            temp = [' '.join(n_grams) for n_grams in ngrams(x.split(' '), i) if ' '.join(n_grams) in brands] \n",
    "        if len(temp)>0:\n",
    "            nameList.append(temp)\n",
    "        if len(nameList)>0:\n",
    "            return nameList[0]\n",
    "        else:\n",
    "            return ''\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "### Cleaning Text fields ###\n",
    "def cleanText(text):\n",
    "    try:\n",
    "        text = ' '.join([w for w in text.split()[:MAX_NUM_WORDS]])\n",
    "        text = text.lower()\n",
    "        text = re.sub(u\"\\u2019\", u\"'\", text)\n",
    "        text = re.sub(u\"\\xed\", u\"i\", text)\n",
    "        text = re.sub(u\"w\\/\", u\" with \", text)\n",
    "        text = re.sub(u\"é\", u\"e\", text)\n",
    "        text = re.sub(u\"ē\", u\"e\", text)\n",
    "        text = re.sub(u\"è\", u\"e\", text)\n",
    "        text = re.sub(u\"ê\", u\"e\", text)\n",
    "        text = re.sub(u\"à\", u\"a\", text)\n",
    "        text = re.sub(u\"â\", u\"a\", text)\n",
    "        text = re.sub(u\"ô\", u\"o\", text)\n",
    "        text = re.sub(u\"ō\", u\"o\", text)\n",
    "        text = re.sub(u\"ü\", u\"u\", text)\n",
    "        text = re.sub(u\"ï\", u\"i\", text)\n",
    "        text = re.sub(u\"ç\", u\"c\", text)\n",
    "        text = re.sub(u\"[^a-z0-9]\", \" \", text)\n",
    "        text = u\" \".join(re.split(\"(\\d+)\", text))\n",
    "        text = re.sub(u\"\\s+\", u\" \", text).strip()\n",
    "        text = \"\".join(text)\n",
    "    except:\n",
    "        text = \"\"\n",
    "    return text\n",
    "\n",
    "def encoding(df, train):\n",
    "    if train:\n",
    "        cols = ['brand_name', 'primary_category', 'sub_category1', 'sub_category2']\n",
    "        for col in cols:\n",
    "            temp = df[col].unique()\n",
    "            label_encoder = LabelEncoder()\n",
    "            df[col] = label_encoder.fit_transform(df[col])\n",
    "            label_dict[col] = (label_encoder, temp)\n",
    "    else:\n",
    "        cols = ['brand_name', 'primary_category', 'sub_category1', 'sub_category2']\n",
    "        for col in cols:\n",
    "            idx = label_dict[col][1]\n",
    "            df.loc[~df[col].isin(idx), col] = ''\n",
    "            df[col] = label_dict[col][0].transform(df[col])\n",
    "    print(\"*******Finished encoding******\")\n",
    "            \n",
    "def remove_stopWords(text):\n",
    "    text = ' '.join([word for word in text.split() if not word in stopWords])\n",
    "    return text\n",
    "\n",
    "def preprocessData(df, train):\n",
    "    print(\"##################Preparing data#############\")\n",
    "    #df = readData()\n",
    "    fillMissingData(df)\n",
    "    df.name = df.name.apply(cleanText)\n",
    "    df.brand_name = df.brand_name.apply(cleanText)\n",
    "    df.item_description = df.item_description.apply(cleanText)\n",
    "    df = splittingCategories(df)\n",
    "    df.loc[df.brand_name == \"\", 'brand_name'] = df.loc[df.brand_name == \"\", 'name'].apply(fillBrandNames)\n",
    "     \n",
    "    for col in ['name', 'category_name', 'brand_name', 'item_description', 'primary_category', 'sub_category1', 'sub_category2']:\n",
    "        df[col] = df[col].astype(str)\n",
    "    \n",
    "    ### remove stop words from name and description\n",
    "    for col in ['name', 'item_description']:\n",
    "        df[col] = df[col].apply(remove_stopWords)\n",
    "        \n",
    "    ### create a new col by adding pri_cat + sub_cat1 + sub_cat2 + brandname + name + description[0:5]\n",
    "    ### remove extra spaces n strip\n",
    "    df['text_info'] = df.primary_category +\" \"+ df.sub_category1 +\" \"+ df.sub_category2 +\" \"+ df.brand_name +\" \"+ df.name\n",
    "    df.text_info = df.text_info +\" \"+df.item_description.apply(lambda x : \" \".join(x.split()[:5]))\n",
    "    df.text_info = df.text_info.apply(lambda x : re.sub(u\"\\s+\", u\" \", x).strip())\n",
    "    \n",
    "    # label encode brand name, category name, primary cat, sub cat 1, sub cat 2\n",
    "    encoding(df, train=True)\n",
    "    \n",
    "    ### change item condition id to 0 to 1 range\n",
    "    df.item_condition_id = df.item_condition_id/5.0\n",
    "    print(\"#########Preprocessed Data#########\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tBpLVGp9TvCD"
   },
   "outputs": [],
   "source": [
    "def createOneVocab(df):\n",
    "    for col in ['name', 'item_description']:\n",
    "        vTemp = [x.split() for x in df.name]\n",
    "        vocab = [elem for listOfList in vTemp for elem in listOfList]\n",
    "    ### remove all words whose freq is less than 3\n",
    "    freq = Counter(vocab)\n",
    "    vocab = list(set([elem for elem in vocab if freq[elem]>3]))\n",
    "    ### delete all the words whose length is less than 3\n",
    "    print(\"*********Finished creating 1-gram vocab***************\")\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8C8nYuPfmo2A"
   },
   "outputs": [],
   "source": [
    "########## Experimenting with Bigrams ################\n",
    "# from itertools import combinations\n",
    "# def getBiGrams(sentence):\n",
    "#     try:\n",
    "#         sentence = np.unique(sentence.split())\n",
    "#         bigramList = []\n",
    "#         for comb in combinations(sentence,2):\n",
    "#             c1 = comb[0] + comb[1]\n",
    "#             c2 = comb[1] + comb[0]\n",
    "#             alreadyInList = False\n",
    "#             if c1 in vocab_onegram:\n",
    "#               newWord = c1\n",
    "#             if c2 in vocab_onegram:\n",
    "#               newWord = c2\n",
    "#             if alreadyInList == False:\n",
    "#               newWord = c1\n",
    "#             if len(c1)>0:\n",
    "#               bigramList.append(newWord)\n",
    "#         return ' '.join(bigramList)\n",
    "#     except:\n",
    "#         return ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "UH3KZkqgTyXN",
    "outputId": "b4492c43-a2fe-4aa6-b338-7c6edd7e29ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-bfa5df341b45>\", line 2, in <module>\n",
      "    originalData = shuffle(originalData, random_state=0)\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 403, in shuffle\n",
      "    return resample(*arrays, **options)\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 327, in resample\n",
      "    resampled_arrays = [safe_indexing(a, indices) for a in arrays]\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 327, in <listcomp>\n",
      "    resampled_arrays = [safe_indexing(a, indices) for a in arrays]\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 205, in safe_indexing\n",
      "    return X.iloc[indices]\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\", line 1478, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\", line 2091, in _getitem_axis\n",
      "    return self._get_list_axis(key, axis=axis)\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\", line 2070, in _get_list_axis\n",
      "    return self.obj._take(key, axis=axis)\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\", line 2789, in _take\n",
      "    verify=True)\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\", line 4533, in take\n",
      "    if ((indexer == -1) | (indexer >= n)).any():\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\", line 43, in _any\n",
      "    return umr_any(a, axis, dtype, out, keepdims)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2018, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\inspect.py\", line 1500, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\inspect.py\", line 1458, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\nitis\\Anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "originalData = readData()\n",
    "originalData = shuffle(originalData, random_state=0)\n",
    "originalData.reset_index(inplace=True, drop=True)\n",
    "y_original = originalData.price\n",
    "X_original = originalData\n",
    "X_train_original, X_test_original, y_train_original, y_test_original = train_test_split(X_original, y_original, test_size=0.3, random_state=42)\n",
    "\n",
    "brands = X_train_original.groupby('brand_name').size()\n",
    "stopWords = stopwords.words('english')\n",
    "label_dict = dict()\n",
    "\n",
    "X_train_preprocessed = preprocessData(X_train_original, train=True)\n",
    "# =============================================================================\n",
    "y_train_scaled = np.log1p(y_train_original)\n",
    "# =============================================================================\n",
    "################################Data Preprocessed#############################################\n",
    "### vocabulary for vectorization\n",
    "vocab_onegram = createOneVocab(X_train_preprocessed)\n",
    "\n",
    "\n",
    "# X_train_preprocessed['bigrams'] = X_train_preprocessed['text_info'].apply(getBiGrams)\n",
    "# vocab_bigram = createBigramVocab(X_train_preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "BBJwaOpkUFe9",
    "outputId": "994a8e0b-5d94-4fa7-9b79-2bc617923bd3"
   },
   "outputs": [],
   "source": [
    "### Caluculate the mean price for each category and brand name and save it in a dictionary\n",
    "# Can implement the grouping with all the cols combined\n",
    "meanPriceDict = {}\n",
    "for col in ['primary_category', 'sub_category1', 'sub_category2', 'brand_name']:\n",
    "    meanPriceDict[col] = X_train_preprocessed.groupby('primary_category')['price'].mean()\n",
    "    meanPriceDict[col] /= max(meanPriceDict[col])\n",
    "    X_train_preprocessed['MeanPrice_'+col] = X_train_preprocessed[col].map(meanPriceDict[col])\n",
    "    X_train_preprocessed['MeanPrice_'+col].fillna(meanPriceDict[col].mean(), inplace=True)\n",
    "    \n",
    "def tokenize(text):\n",
    "    return [word for word in text.split()]\n",
    "\n",
    "### Vectorizing the text\n",
    "nameVectorizer = TfidfVectorizer(vocabulary = vocab_onegram, tokenizer=tokenize)\n",
    "vectorizedName = nameVectorizer.fit_transform(X_train_preprocessed.name)\n",
    "print(vectorizedName.shape)\n",
    "itemVectorizer = TfidfVectorizer(vocabulary=vocab_onegram, tokenizer=tokenize)\n",
    "vectorizedItem = itemVectorizer.fit_transform(X_train_preprocessed.item_description)\n",
    "\n",
    "# Text info vectorized\n",
    "textInfoVectorizer = TfidfVectorizer(tokenizer=tokenize, ngram_range=(1,2), max_features=40000, min_df=3)\n",
    "vectorizedTextInfo = textInfoVectorizer.fit_transform(X_train_preprocessed.text_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hndxRGzIVqLt",
    "outputId": "6d1ca0dd-b587-4553-b863-7af3e3e56ca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix size  (1037774, 86538)\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "colsToKeep = ['item_condition_id', 'shipping', 'MeanPrice_primary_category',\n",
    "              'primary_category', 'sub_category1', 'sub_category2', 'brand_name',\n",
    "              'MeanPrice_sub_category1', 'MeanPrice_sub_category2', 'MeanPrice_brand_name']\n",
    "# =============================================================================\n",
    "y_train = y_train_scaled.values\n",
    "# y_train = y_train_original.values\n",
    "# =============================================================================\n",
    "X_train = hstack((X_train_preprocessed[colsToKeep].values, vectorizedName, vectorizedItem, vectorizedTextInfo)).tocsr()\n",
    "# X_train = hstack((X_train_preprocessed[colsToKeep].values, vectorizedTextInfo)).tocsr()\n",
    "print(\"Train matrix size \",X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "S_9C-oTtV1fA",
    "outputId": "6e41a562-400c-42de-ce0d-b65e10c8e1b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************Starting Model Training*************\n",
      "***************Linear Regression****************\n",
      "RMSLE is  0.4231795075211465\n",
      "R squared value  0.5317033003644911\n"
     ]
    }
   ],
   "source": [
    "####### Linear Regression #########\n",
    "print(\"************Starting Model Training*************\")\n",
    "linReg = LinearRegression()\n",
    "linReg.fit(X_train, y_train)\n",
    "y_pred = linReg.predict(X_train)\n",
    "print(\"***************Linear Regression****************\")\n",
    "print(\"RMSLE is \",rmsle(y_pred, y_train))\n",
    "print(\"R squared value \",r2_score(y_pred, y_train))\n",
    "\n",
    "\n",
    "# ridge = Ridge(alpha=1.0)\n",
    "# ridge.fit(X_train, y_train)\n",
    "# y_pred = ridge.predict(X_train)\n",
    "# print(\"***************Ridge Regression****************\")\n",
    "# print(\"RMSLE is \",rmsle(y_pred, y_train))\n",
    "\n",
    "\n",
    "# lasso = Lasso(alpha=1.0)\n",
    "# lasso.fit(X_train, y_train)\n",
    "# y_pred = lasso.predict(X_train)\n",
    "# print(\"***************Lasso Regression****************\")\n",
    "# print(\"RMSLE is \",rmsle(y_pred, y_train))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NXTBJqWqWWGb"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "params = {\n",
    "        'learning_rate': 0.57,\n",
    "        'application': 'regression',\n",
    "        'max_depth': 5,\n",
    "        'num_leaves': 32,\n",
    "        'verbosity': -1,\n",
    "        'metric': 'RMSE',\n",
    "        'data_random_seed': 1,\n",
    "        'bagging_fraction': 0.6,\n",
    "        'bagging_freq': 5,\n",
    "        'feature_fraction': 0.65,\n",
    "        'nthread': 4,\n",
    "        'min_data_in_leaf': 100,\n",
    "        'max_bin': 31\n",
    "}\n",
    "d_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgmb = lgb.train(params, train_set=d_train, num_boost_round=3000, verbose_eval=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "DKHSPpifX18j",
    "outputId": "9f6faa07-1b54-4068-fef8-b70f436e8d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********Light GBM**********\n",
      "RMSLE is  0.4359617248095056\n",
      "R squared value  0.49459254868020686\n"
     ]
    }
   ],
   "source": [
    "y_pred = lgmb.predict(X_train)\n",
    "print(\"***********Light GBM**********\")\n",
    "print(\"RMSLE is \",rmsle(y_pred, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "cbL4TtqBikdJ",
    "outputId": "0523de34-2372-4c7a-e53a-f06b8030e676"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################Preparing data#############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:6287: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5208: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******Filled Missing Data*******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******Finished encoding******\n",
      "#########Preprocessed Data#########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(444761, 23264)\n",
      "(444761, 23264)\n",
      "(444761, 40000)\n"
     ]
    }
   ],
   "source": [
    "# Preparing Test Data\n",
    "\n",
    "X_train_original, X_test_original, y_train_original, y_test_original = train_test_split(X_original, y_original, test_size=0.3, random_state=42)\n",
    "\n",
    "label_dict = dict()\n",
    "\n",
    "X_test_preprocessed = preprocessData(X_test_original, train=False)\n",
    "# =============================================================================\n",
    "y_test_scaled = np.log1p(y_test_original)\n",
    "# =============================================================================\n",
    "################################Data Preprocessed#############################################\n",
    "\n",
    "\n",
    "for col in ['primary_category', 'sub_category1', 'sub_category2', 'brand_name']:\n",
    "    X_test_preprocessed['MeanPrice_'+col] = X_test_preprocessed[col].map(meanPriceDict[col])\n",
    "    X_test_preprocessed['MeanPrice_'+col].fillna(meanPriceDict[col].mean(), inplace=True)\n",
    "\n",
    "\n",
    "### Vectorizing the text\n",
    "vectorizedNameTest = nameVectorizer.transform(X_test_preprocessed.name)\n",
    "print(vectorizedNameTest.shape)\n",
    "vectorizedItemTest = itemVectorizer.transform(X_test_preprocessed.item_description)\n",
    "print(vectorizedItemTest.shape)\n",
    "\n",
    "vectorizedTextInfoTest = textInfoVectorizer.transform(X_test_preprocessed.text_info)\n",
    "print(vectorizedTextInfoTest.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6moSWa0qBooa",
    "outputId": "c35c49c8-d0e9-4c54-bc09-7a9f70c11a31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test matrix size  (444761, 86538)\n"
     ]
    }
   ],
   "source": [
    "y_test = y_test_scaled.values\n",
    "X_test = hstack((X_test_preprocessed[colsToKeep].values, vectorizedNameTest, vectorizedItemTest, vectorizedTextInfoTest)).tocsr()\n",
    "# X_test = hstack((X_test_preprocessed[colsToKeep].values, vectorizedTextInfoTest)).tocsr()\n",
    "\n",
    "print(\"Test matrix size \",X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "7soOAuh8CcWQ",
    "outputId": "46b3c508-8751-484b-f0b0-eaeda1ecb48e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************Starting Model Training*************\n",
      "***************Linear Regression****************\n",
      "RMSLE is  0.4667819772888975\n"
     ]
    }
   ],
   "source": [
    "####### Linear Regression Test Score #########\n",
    "print(\"************Starting Model Training*************\")\n",
    "y_pred = linReg.predict(X_test)\n",
    "print(\"***************Linear Regression****************\")\n",
    "print(\"RMSLE is \",rmsle(y_pred, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "RHbf-GvNDviS",
    "outputId": "ce57dc8c-1784-4b75-9fc4-0400847aef57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********Light GBM**********\n",
      "RMSLE is  0.47105998281877476\n"
     ]
    }
   ],
   "source": [
    "####### Light GBM Test Score #########\n",
    "y_pred = lgmb.predict(X_test)\n",
    "print(\"***********Light GBM**********\")\n",
    "print(\"RMSLE is \",rmsle(y_pred, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ADMv1.0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
